"""
Reinforcement Learning environment for test refinement.

Defines the state space, action space, and step logic for the test refinement task.
"""

import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

from ..evaluation.metrics import TestMetrics, TestQualityEvaluator
from ..rl_env.reward import RewardFunction, RewardConfig
from ..static_analyzer.extractor import JavaAnalyzer

logger = logging.getLogger(__name__)


@dataclass
class TestRefinementState:
    """State representation for test refinement."""
    # Original test (from EvoSuite)
    original_test_code: str
    original_metrics: Optional[TestMetrics] = None
    
    # Source code under test
    source_code: str
    source_class_name: str
    
    # Context information
    class_info: Optional[Dict] = None
    package: str = ""
    imports: List[str] = None
    
    # Current refinement attempt
    current_refined_code: Optional[str] = None
    current_metrics: Optional[TestMetrics] = None
    
    def __post_init__(self):
        if self.imports is None:
            self.imports = []
    
    def to_prompt(self) -> str:
        """
        Convert state to prompt for LLM.
        
        Returns:
            Prompt string for the LLM
        """
        prompt = f"""You are an expert Java test engineer. Your task is to refine the following unit test to improve its maintainability while preserving its effectiveness.

## Source Code Under Test:
```java
{self.source_code}
```

## Original Test (Generated by EvoSuite):
```java
{self.original_test_code}
```

## Refinement Goals:
1. **Improve Maintainability:**
   - Reduce test smells (e.g., Assertion Roulette, Eager Test, Mystery Guest)
   - Reduce cyclomatic complexity
   - Improve readability and clarity
   - Use meaningful variable names and comments where helpful

2. **Preserve Effectiveness:**
   - Maintain or improve branch coverage
   - Ensure all assertions are meaningful
   - Keep the test comprehensive

## Instructions:
- Generate a refined version of the test
- The refined test MUST compile and pass
- Keep the same test structure but improve quality
- Use clear, descriptive names
- Add comments only where they add value
- Ensure proper assertions with messages

## Refined Test:
```java"""
        
        return prompt


class TestRefinementEnvironment:
    """
    RL Environment for test refinement task.
    
    This environment wraps the test refinement problem as an RL task where:
    - State: Original test + source code + context
    - Action: Refined test code (generated by LLM)
    - Reward: Quality improvement (from reward function)
    """
    
    def __init__(
        self,
        project_dir: Path,
        evaluator: TestQualityEvaluator,
        reward_function: RewardFunction,
        analyzer: Optional[JavaAnalyzer] = None
    ):
        """
        Initialize test refinement environment.
        
        Args:
            project_dir: Project directory for compilation/testing
            evaluator: Test quality evaluator
            reward_function: Reward function
            analyzer: Java code analyzer
        """
        self.project_dir = project_dir
        self.evaluator = evaluator
        self.reward_function = reward_function
        self.analyzer = analyzer or JavaAnalyzer()
        
        self.current_state: Optional[TestRefinementState] = None
    
    def reset(
        self,
        original_test_code: str,
        source_code: str,
        source_class_name: str,
        target_classes: Optional[List[str]] = None
    ) -> TestRefinementState:
        """
        Reset environment with a new test refinement task.
        
        Args:
            original_test_code: Original test from EvoSuite
            source_code: Source code under test
            source_class_name: Name of source class
            target_classes: Classes to test (for mutation testing)
            
        Returns:
            Initial state
        """
        # Extract class information from source code
        class_info = self.analyzer.get_code_summary(source_code)
        
        # Extract package and imports from original test
        package, imports = self._extract_package_imports(original_test_code)
        
        # Evaluate original test
        test_class_name = self._extract_test_class_name(original_test_code)
        original_metrics = self.evaluator.evaluate_test(
            original_test_code,
            test_class_name,
            target_classes,
            run_mutation=False  # Skip mutation for original to save time
        )
        
        # Create initial state
        self.current_state = TestRefinementState(
            original_test_code=original_test_code,
            original_metrics=original_metrics,
            source_code=source_code,
            source_class_name=source_class_name,
            class_info=class_info,
            package=package,
            imports=imports
        )
        
        logger.info(f"Environment reset for {source_class_name}")
        logger.info(f"Original metrics: {original_metrics.to_dict()}")
        
        return self.current_state
    
    def step(
        self,
        refined_test_code: str,
        target_classes: Optional[List[str]] = None,
        run_mutation: bool = False
    ) -> Tuple[TestRefinementState, float, bool, Dict]:
        """
        Take a step in the environment by evaluating a refined test.
        
        Args:
            refined_test_code: Refined test code (action)
            target_classes: Classes under test
            run_mutation: Whether to run mutation testing
            
        Returns:
            Tuple of (new_state, reward, done, info)
        """
        if self.current_state is None:
            raise RuntimeError("Environment not initialized. Call reset() first.")
        
        # Extract test class name
        test_class_name = self._extract_test_class_name(refined_test_code)
        
        # Evaluate refined test
        refined_metrics = self.evaluator.evaluate_test(
            refined_test_code,
            test_class_name,
            target_classes,
            run_mutation=run_mutation
        )
        
        # Calculate reward
        reward = self.reward_function.calculate_reward(
            self.current_state.original_metrics,
            refined_metrics
        )
        
        # Update state
        self.current_state.current_refined_code = refined_test_code
        self.current_state.current_metrics = refined_metrics
        
        # Episode is always done after one step (single refinement)
        done = True
        
        # Additional info
        info = {
            'original_metrics': self.current_state.original_metrics.to_dict(),
            'refined_metrics': refined_metrics.to_dict(),
            'reward_breakdown': self.reward_function.get_detailed_feedback(
                self.current_state.original_metrics,
                refined_metrics
            )
        }
        
        logger.info(f"Step completed. Reward: {reward:.4f}")
        logger.debug(f"Refined metrics: {refined_metrics.to_dict()}")
        
        return self.current_state, reward, done, info
    
    def _extract_package_imports(self, code: str) -> Tuple[str, List[str]]:
        """
        Extract package and imports from Java code.
        
        Args:
            code: Java source code
            
        Returns:
            Tuple of (package, imports)
        """
        import re
        
        package = ""
        imports = []
        
        # Extract package
        package_match = re.search(r'package\s+([\w.]+);', code)
        if package_match:
            package = package_match.group(1)
        
        # Extract imports
        import_matches = re.findall(r'import\s+([\w.*]+);', code)
        imports = import_matches
        
        return package, imports
    
    def _extract_test_class_name(self, code: str) -> str:
        """
        Extract test class name from code.
        
        Args:
            code: Java test code
            
        Returns:
            Test class name
        """
        import re
        
        # Look for public class declaration
        match = re.search(r'public\s+class\s+(\w+)', code)
        if match:
            return match.group(1)
        
        # Fallback: any class declaration
        match = re.search(r'class\s+(\w+)', code)
        if match:
            return match.group(1)
        
        return "UnknownTest"
    
    def get_observation(self) -> str:
        """
        Get current observation as prompt for LLM.
        
        Returns:
            Prompt string
        """
        if self.current_state is None:
            raise RuntimeError("Environment not initialized.")
        
        return self.current_state.to_prompt()
    
    def render(self) -> str:
        """
        Render current state for debugging.
        
        Returns:
            Human-readable state description
        """
        if self.current_state is None:
            return "Environment not initialized."
        
        output = []
        output.append("=" * 80)
        output.append("TEST REFINEMENT ENVIRONMENT STATE")
        output.append("=" * 80)
        output.append(f"\nSource Class: {self.current_state.source_class_name}")
        output.append(f"Package: {self.current_state.package}")
        
        if self.current_state.original_metrics:
            output.append("\nOriginal Test Metrics:")
            for key, value in self.current_state.original_metrics.to_dict().items():
                output.append(f"  {key}: {value}")
        
        if self.current_state.current_metrics:
            output.append("\nRefined Test Metrics:")
            for key, value in self.current_state.current_metrics.to_dict().items():
                output.append(f"  {key}: {value}")
            
            # Calculate improvements
            reward = self.reward_function.calculate_reward(
                self.current_state.original_metrics,
                self.current_state.current_metrics
            )
            output.append(f"\nReward: {reward:.4f}")
        
        output.append("=" * 80)
        
        return "\n".join(output)


class BatchTestRefinementEnvironment:
    """
    Batch version of test refinement environment.
    
    Handles multiple test refinement tasks in parallel for efficient training.
    """
    
    def __init__(
        self,
        project_dirs: List[Path],
        evaluator: TestQualityEvaluator,
        reward_function: RewardFunction,
        analyzer: Optional[JavaAnalyzer] = None
    ):
        """
        Initialize batch environment.
        
        Args:
            project_dirs: List of project directories
            evaluator: Test quality evaluator
            reward_function: Reward function
            analyzer: Java code analyzer
        """
        self.environments = [
            TestRefinementEnvironment(proj_dir, evaluator, reward_function, analyzer)
            for proj_dir in project_dirs
        ]
        self.num_envs = len(self.environments)
    
    def reset(
        self,
        test_data: List[Dict]
    ) -> List[TestRefinementState]:
        """
        Reset all environments with batch data.
        
        Args:
            test_data: List of dicts with test information
            
        Returns:
            List of initial states
        """
        states = []
        for env, data in zip(self.environments, test_data):
            state = env.reset(
                data['original_test_code'],
                data['source_code'],
                data['source_class_name'],
                data.get('target_classes')
            )
            states.append(state)
        
        return states
    
    def step(
        self,
        refined_tests: List[str],
        target_classes_list: Optional[List[List[str]]] = None,
        run_mutation: bool = False
    ) -> Tuple[List[TestRefinementState], List[float], List[bool], List[Dict]]:
        """
        Step all environments.
        
        Args:
            refined_tests: List of refined test codes
            target_classes_list: List of target classes for each test
            run_mutation: Whether to run mutation testing
            
        Returns:
            Tuple of (states, rewards, dones, infos)
        """
        if target_classes_list is None:
            target_classes_list = [None] * len(refined_tests)
        
        results = [
            env.step(test, target_classes, run_mutation)
            for env, test, target_classes in zip(
                self.environments,
                refined_tests,
                target_classes_list
            )
        ]
        
        states, rewards, dones, infos = zip(*results)
        
        return list(states), list(rewards), list(dones), list(infos)
    
    def get_observations(self) -> List[str]:
        """Get observations from all environments."""
        return [env.get_observation() for env in self.environments]
